library(gcdnet)
library(GMCM)
library(matrixStats)
library(stats)  
library(epicalc)
library(Hmisc)
library(Epi)
library(mgcv)



d1<-read.csv("dataset.csv")

d1<-na.omit(d1) #ensure your is complete with no missing observations

###########################################################################
### Using adaptive elastic net to create environmental risk score (ERS) ###
###########################################################################

##for data pre-processing: 
  ## make sure all of your categorical covariates are changed to binary dummy variables
    ## exclude the reference level, example: if BMI is 3-levels, do not include the lower reference binary variable
  ## normalize your exposures either using log-transformation or standardization 
##Function components: 
  ##ind.data is the dataset which contains your predictors and covariates
  ##depen.data is the dataset which contains your outcome variable
  ##preds is the number of predictors in your dataset: 
    ## defines variables in ind.data that WILL be penalized in ENET
  ##covars is the number of covariates in your dataset: 
    ## defines variables in ind.data that WILL NOT be penalized in ENET
  ##a is the range of covariates in ind.data - defines variables that WILL NOT be penalized in aENET
  ##b extracts the range of rows from aENET output to be used for ERS - should only exclude first row (intercept)

aENET_fun = function(ind.data, depen.data, preds, covars, a){
  ind.data<-data.matrix(ind.data)
  depen.data<-data.matrix(depen.data)
  #set a seed of random numbers
  set.seed(111) 
  
  #five fold cross-validation list for estimating lambda 2 tuning parameter
  num.obs=nrow(ind.data)
  foldid=sample(rep(seq(5),length=nrow(ind.data))) 
  lambda2.list=seq(0,1,by=0.01) #candidate lambda2 values
  num.lambda2=length(lambda2.list)
  min.cv=numeric(num.lambda2)
  
  #creates two vectors of penalty factors that allows shrinkage of exposures, but not covariates
  pf_en<-c(rep(1,preds),rep(0,covars)) 
  pf_en_2<-c(rep(1,preds),rep(0,covars))
  
  # loop that runs each lambda2 value in the cv.gcdnet function to test for the optimal (minimum CV error) lambda2 value  
  for(i in 1:num.lambda2){
    cv.en=cv.gcdnet(ind.data, depen.data, foldid=foldid, lambda2=lambda2.list[i], 
                    method="ls", pred.loss="loss", pf=pf_en, pf2=pf_en_2, standardize=T)  
    # lambda (for L1 penalty) will be automatically generated by default setting
    min.cv[i]=min(cv.en$cvm)  # collects minimum cv error for each lambda2 candidate
  }
  
  lambda2.opt=lambda2.list[which.min(min.cv)] #define optimal lambda2 that has the minimum CV error
  print(lambda2.opt)
  # now creating potential lambda1 tuning parameter 
  lambda.list=cv.en$lambda #candidate lambda (for L1 penalty)
  
  #cross validation for lambda given the optimal lambda2
  par(mar=c(5,5,4,2))
  cv.en=cv.gcdnet(ind.data, depen.data, foldid=foldid, lambda2=lambda2.opt, 
                  method="ls", pred.loss="loss", pf=pf_en, pf2=pf_en_2, standardize=T) 
  plot(cv.en)
  minval=cv.en$lambda.min
  fit.en = gcdnet(ind.data, depen.data, lambda=minval, standardize=T, 
                  lambda2=lambda2.opt, method="ls", pf=pf_en, pf2=pf_en_2)
  beta.en = coef(fit.en)
  
  ### Adaptive ENET
  v <- log(ncol(ind.data))/log(num.obs)
  gamma <- ceiling(2*v/(1-v))+1
  x.sd <- colSds(as.matrix(ind.data))*((num.obs-1)/num.obs)^0.5 # sample sd for each predictor 
  beta.enet.star <- beta.en[-1,]*x.sd
  
  ada.wts <- (abs(beta.enet.star)+1/num.obs)^(-gamma)
  ada.wts[a] <- 0
  min.cv <- numeric(num.lambda2)
  # CV for each candidate lambda2
  for(i in 1:num.lambda2){
    cv.adenet = cv.gcdnet(ind.data, depen.data, foldid=foldid, lambda2=lambda2.list[i],
                          method="ls", pf=ada.wts, pred.loss="loss", standardize=T)
    # lambda (for L1 penalty) will be automatically generated by default setting
    min.cv[i] = min(cv.adenet$cvm)  # collect minimum cv error for each lambda2 candidate
  }
  
  lambda2.adenet.opt=lambda2.list[which.min(min.cv)] 
  
  #cross validation for lambda given the optimal lambda2
  cv.adenet <- cv.gcdnet(ind.data, depen.data, foldid=foldid, lambda2=lambda2.adenet.opt,
                         method="ls", pf=ada.wts, pred.loss="loss", standardize=T) 
  plot(cv.adenet)
  minval = cv.adenet$lambda.min
  fit.all <- gcdnet(ind.data, depen.data, lambda=minval, 
                    lambda2=lambda2.adenet.opt, pf=ada.wts, method="ls", standardize=T ) 
  beta.all <- as.matrix(coef(fit.all))# get the coefficients
  beta.all
  
  ##standard error and p-value for each non-zero estimated coefficient
  index.a=which(beta.all[-1]!=0)#the coordinate # of the non-zero beta.adenet except the intercept
  p.ne0=length(index.a)#the number of non-zero estimated-effect predicators
  x.wi=as.matrix(cbind(1,ind.data)) #the design matrix with first column being 1
  sigma.2=t(depen.data-x.wi%*%beta.all)%*%(depen.data-x.wi%*%beta.all)/(num.obs-p.ne0-1)#estimated sigma squared
  
  x.mean=colMeans(ind.data)
  x.stdi=matrix(0,num.obs,ncol(ind.data))#standardized x
  colnames(x.stdi)=colnames(ind.data)
  for(i in 1:ncol(ind.data)) {
    x.stdi[,i]=(ind.data[,i]-x.mean[i])/x.sd[i]  # standardized x
  }
  x.stdi.a=x.stdi[,index.a]  #standardized x matrix for non-zero estimated-effect predicators
  Sig.a.stdi=t(x.stdi.a)%*%x.stdi.a  #Sigma_A in the Theorem 3.3 of Zou and Zhang (2009)      
  
  ##variance matrix for the non-zero estimated coefficents for standardized predicators
  var.a.stdi=as.numeric(sigma.2)*(1+lambda2.adenet.opt/2)^2*solve(Sig.a.stdi+diag(rep(num.obs*lambda2.adenet.opt,p.ne0))+ (num.obs*lambda2.adenet.opt/2)^2*solve(Sig.a.stdi))
  
  x.sd.a.inv=diag(1/x.sd[index.a])
  var.a.ori=x.sd.a.inv%*%var.a.stdi%*%t(x.sd.a.inv)   #variance matrix for the non-zero estimated coefficents for original predicators
  var.int=t(as.matrix(x.mean[index.a]))%*%var.a.ori%*%as.matrix(x.mean[index.a]) # variance for the intercept
  
  name.x.a=c("(Intercept)",colnames(ind.data)[index.a])# the names of variabls have non-zero estimated coefficient including intercept
  se.a=data.frame(se=sqrt(c(var.int,diag(var.a.ori))),row.names=name.x.a) #Standard error of non-zero beta.adenet including the intercept
  
  pvalue.a=data.frame(pvalue=2-2*apply(abs(beta.all[c(1,index.a+1)])/se.a,1,pnorm),row.names=name.x.a)
  
  result.adenet=cbind(numeric(ncol(ind.data)+1),matrix(NA,ncol(ind.data)+1,2))
  colnames(result.adenet)=c("beta","SE","p-value")
  rownames(result.adenet)=c("(Intercept)",colnames(ind.data))
  result.adenet[name.x.a,1]=beta.all[name.x.a,]
  result.adenet[name.x.a,2]=se.a[name.x.a,]
  result.adenet[name.x.a,3]=pvalue.a[name.x.a,]
  result.adenet = as.data.frame(result.adenet)
  result.adenet$loci = result.adenet$beta - 1.96*result.adenet$SE
  result.adenet$hici = result.adenet$beta + 1.96*result.adenet$SE
  return(result.adenet)
}

results<-aENET_fun(ind.data, depen.data, preds, covars, a)

#Extract enet beta coefficients for weights
nexp<- #define the number of exposures
weights <- (results[,1])[2:(nexp+1)]

#isolate your log-transformed or standardized exposure variables
exp_mat <- as.matrix(d1[,colnames(d1) %in% rownames(results)[2:(nexp+1)]])

#Construct environmental risk score by applying linear combination of weights and exposure matrix
ers <- exp_mat%*%as.matrix(weights, ncol = 1)

#Combine the ERS to your dataset
d1 <- cbind(d1, ers)

# Now that ERS is added to your dataset, the user can either:
# (1) Conduct pairwise mediation anlaysis with single mediators using script 1a_pairwise_mediation.R
# or
# (2) Conduct pairwise mediation analysis with multiple meidators using script 2_mediator_shrinkage_reduction.R 




